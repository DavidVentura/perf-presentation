<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>

    <link rel="stylesheet" href="dist/reset.css">
    <link rel="stylesheet" href="dist/reveal.css">
    <link rel="stylesheet" href="dist/theme/white.css">
    <link rel="stylesheet" href="plugin/chalkboard/style.css">
    <link rel="stylesheet" href="css/all.min.css">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css">
    <style type="text/css">
       img { max-height: 45vh !important; }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Linux tuning intro
                  
                  * Bit of theory
                  * Poking at a server
                  * Another bit of theory

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Main topics
                  
                  * Low latency and why it matters
                  * Peak vs consistent performance
                  * Designing with the hardware in mind

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Why low latency matters

                  ![](img/optimization_before.png)

                  Note:
                  In green: what we want to do. can't be optimized away.
                  In yellow/orange: tasks that _must_ be done to achieve the goal.

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Why low latency matters
                  ![](img/optimization_benefits.png)

                  Note:
                  Reducing latency/overhead is relevant because we can calculate
                  a better price in the same amnt of time, or reduce the time it takes
                  to publish the price

  we don't do this by maximizing peak performance

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Low latency tuning intro
                  
                  * Non-uniform memory architecture (NUMA)
                  * Minimizing jitter
                  * Userspace networking (EFVI)

                  Note:

                  on these topics i need to explain first the defaults, to then
                  show the reasoning behind the changes we do
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Unified memory architecture
                  ![](img/uma.png)

                  Note:
                  This is what the memory layout looks for devices with a single CPU
                  like your desktop pc
                  *hanging off the io controller is pci-e*
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### NON-Uniform memory architecture
                  ![](img/numa.png)

                  Note:
                  This is what the memory layout looks for devices with multiple CPUs
                  like servers

                  *hanging off the io controller is pci-e*
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ![](img/server.jpg)

                  Note:
                  What a server looks like.
                  Show CPU, memory, nic, disks, numa nodes.
                  Draw path of IP packet. (cross-numa)
                  Mention interconnect.
                  Mention we put everything in the OS in the slow node
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ![](img/server-latency.jpg)

                  Note:
                  Going over the interconnect costs 60ns! or 66% extra<sup>[1](https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html)</sup>
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                ![](img/bandwidth-server.jpg)

                Note:
                while the latency is only 60% higher, throughput is hit by 72%
                the interconnect has approx 45GB bandwidth limit, and it's shared by pci-e

          </textarea>
        </section>
        <section>
          <section data-markdown data-separator-notes="Note:">
            <textarea data-template>
                    ## Dealing with NUMA

                    * PCI-e bus locality
                    * Process &Leftrightarrow; memory locality

                    Note:
                    we enforce minimal amount of cross-node communication, or keep
                    only slow components on a path that has to cross the interconnect
            </textarea>
          </section>
          <section data-markdown>
            <textarea data-template>
                    ![](img/server.jpg)
            </textarea>
          </section>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Low latency tuning intro
                  
                  * ~Non-uniform memory architecture (NUMA)~
                  * Minimizing jitter
                  * Userspace networking (EFVI)

                  Note:
                  that's it for NUMA, moving on to jitter
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ## Minimizing jitter

                  * Core isolation
                  * IRQ Affinity
                  * Huge pages



  Note:

  Divided in 3 sub-topics.
  Jitter: Unwanted variance in timing or expected performance
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Jitter
                  ![](img/boosting.png)

                  Note:

Unwanted variance in timing or expected performance

There are reasons for which running our compute at
100% capacity will end up with jittery behavior
we attempt to get to the highest performance that does not 
exhibit this behavior

even tho the peak perf performs more compute over time
a consistent environment reduces risk and simplifies the
life of the developers

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Process scheduling
                  ![](img/no-cpu_pinning.png)
                  Note:


                  * By default, the scheduler will move processes around
                    * higher total system throughput
                    * at the cost of peak performance for any single process
                    * Cache + data locality implications, interrupted = jitter
                  * We can minimize this by disabling the scheduler on cores

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Core isolation
                  ![](img/cpu_pinning.png)
                  Note:
                  * Pinning a single process to a core ensures no interruption + no movement (from scheduler)
                  * The scheduler-induced pauses go from 1~4ms to 0ms on a pinned process
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Core isolation
                  The scheduler-induced pauses (1~4ms) disappear
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### IRQ Load balancing
                  ![](img/irq_balance.png)
                  Note:
                  * Hardware will generate an 'interrupt' == notification to the kernel on most events
                    * the kernel has to account for it and do something
                  * By default the interrupts will be load-balanced across all cores to ensure fairness
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### IRQ Steering
                  ![](img/irq_balance_steering.png)
                  Note:
                  * Steering all IRQs to one core will ensure the others don't have
                    context switches, of course it will deteriorate perf on the sacrifical core
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### IRQ Steering
                  * Pauses up to ~200Î¼s disappear
                  * No context switches
                  * Performance deteriorates on the sacrifical core
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Memory mapping
                  ![](./img/virtual_physical_map.png)

                  Note:
                  While applications see a unified, contiguous memory layout
                  it's not the case. the physical memory is shared and 
                  there is a translation layer, called virtual memory

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Memory mapping: Page table
                  ![](./img/page_table.png)

                  Note:

                  This translation (virtual memory) needs bookkeepping
                  it is done via a 4 level pointer structure called "page table"

                  this happens on _every_ memory access

                  which is 360-600ns

          </textarea>
        </section>

        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Memory mapping: TLB
                  ![](./img/tlb.png)

                  Note:

                  To prevent those 4 random accesses on every page table lookup
                  there's the TLB (translation lookaside buffer) which is basically
                  a cache of the virtual address -> physical address mapping

                  the TLB is *hardware* and has limited capacity for entries, it can fit
                  about 1500 pages.

                  The default page size on x86 is 4KB -> 6MB working set memory
                  -> if you can use larger pages, you can cover a bigger region of
                  the physical space with the same amount of entries


          </textarea>
        </section>

        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Huge pages
                  Increasing system-wide page size (2MB or 1GB)

                  * Fewer allocations
                  * TLB entries cover more backing memory


                  Note:
                  * Fewer allocations
                  * Fewer TLB entries == fewer misses (up to 600ns each!)
                      * 1500 pages x 4KB = 6MB
                      * 1500 pages x 2MB = 3GB
                      * 1500 pages x 1GB = 15TB

                  * Pages are 2MB or 1GB instead of 4KB
                  * Pinned to the same NUMA node

          </textarea>
        </section>

        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ### Low latency tuning intro
                  
                  * ~Non-uniform memory architecture (NUMA)~
                  * ~Minimizing jitter~
                  * Userspace networking (EFVI)

                  Note:
                  that's it for jitter, moving on to userspace networking
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ## Standard networking
                  ![](./img/kernel_space.png)

          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ## Userspace networking

                  ![](./img/userspace.png)


                  Note:
                  In contrast:
                  * No interrupts (dma to userspace)
                  * Hugepages
                  by avoiding the kernel space network and having the card
                  DMA into a user buffer, we can avoid having to block on
                  a syscall per packet

                  Cons: As there are no interrupts, the process needs to poll by itself
                  the shared buffer
          </textarea>
        </section>
        <section data-markdown data-separator-notes="Note:">
          <textarea data-template>
                  ## Conclusions

                  * Resource scheduling / allocation
                  * Consistency 
                  Note:

                  Working with the architecture to minimize overhead, at the cost of
                  efficiently utilizing resources
          </textarea>
        </section>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/chalkboard/plugin.js"></script>
    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        hash: true,
        center: false,
        controls: true,
        transition: 'none',
        chalkboard: { // font-awesome.min.css must be available
          src: "chalkboard/chalkboard.json",
          storage: "chalkboard-demo",
          boardmarkerWidth: 7,
          toggleChalkboardButton: { left: "60px" },
          toggleNotesButton: { left: "100px" },
          colorButtons: 6,
          boardmarkers : [
                  { color: 'rgba(50,50,50,1)', cursor: 'url(' + path + 'img/boardmarker-black.png), auto'},
                  { color: '#10FDEC', cursor: 'url(' + path + 'img/boardmarker-blue.png), auto'},
                  { color: '#10FD6D', cursor: 'url(' + path + 'img/boardmarker-red.png), auto'},
                  { color: '#F1FD10', cursor: 'url(' + path + 'img/boardmarker-green.png), auto'},
                  { color: '#FF003B', cursor: 'url(' + path + 'img/boardmarker-orange.png), auto'},
                  { color: 'rgba(150,0,20150,1)', cursor: 'url(' + path + 'img/boardmarker-purple.png), auto'},
          ],
        },

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealChalkboard, ]
      });
    </script>
  </body>
</html>
